version: '3.8'
services:
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      # - HOST=0.0.0.0
      # - PORT=3000
      - NODE_ENV=development
  backend:
    build: ./backend
    ports:
      - "5000:5000"
    environment:
      - TOKEN=${TOKEN}
    #depends_on:
    #  - llama
  #llama:
    #image: ghcr.io/ggerganov/llama.cpp:full
    #ports:
      #- "8000:8000"
    #volumes:
      #- ./models:/models
    #environment:
      #- MODEL=/models/llama-2-7b.Q4_0.gguf
